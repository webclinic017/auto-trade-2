{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest & DRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "投资是一个系列的决策，寻找是一段时间内的收益率，而不是某一天或某几天的涨与跌。\n",
    "\n",
    "这种多轮博弈，而且带有成本的多轮博弈，使用强化学习再适合不过了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 强化学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入一般常规配置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import plt, mpl\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "\n",
    "#正常显示画图时出现的中文和负号\n",
    "mpl.rcParams['font.sans-serif']=['SimHei']\n",
    "mpl.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "os.environ['PYTHONHASHSEED'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现一个观察空间类，这是一个N维向量，n的维度等于我们因子的个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class observation_space:\n",
    "    def __init__(self, n):\n",
    "        self.shape = (n,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作空间，这里动作空间是2（买，卖）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class action_space:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    def seed(self, seed):\n",
    "        pass\n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模似交易环境：\n",
    "\n",
    "交易环境除了准备交易数据之外，最重要的就是step函数，我们这里的reward仍然与昨天的DNN类似，智能体猜中一次，则奖励加1分。\n",
    "\n",
    "准确率低于47.5%时退出，或者一个episode结束时退出。\n",
    "\n",
    "相当于我们做一个交易游戏，一个智能体进来“猜”，猜中加1分，计算准确率，低于47.5%直接出局。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finance:\n",
    "    url = 'http://hilpisch.com/aiif_eikon_eod_data.csv'\n",
    "    def __init__(self, symbol, features):\n",
    "        self.symbol = symbol\n",
    "        self.features = features\n",
    "        self.observation_space = observation_space(4)\n",
    "        self.osn = self.observation_space.shape[0]\n",
    "        self.action_space = action_space(2)\n",
    "        self.min_accuracy = 0.475\n",
    "        self._get_data()\n",
    "        self._prepare_data()\n",
    "    def _get_data(self):\n",
    "        self.raw = pd.read_csv(self.url, index_col=0,\n",
    "                               parse_dates=True).dropna()\n",
    "    def _prepare_data(self):\n",
    "        self.data = pd.DataFrame(self.raw[self.symbol])\n",
    "        self.data['r'] = np.log(self.data / self.data.shift(1))\n",
    "        self.data.dropna(inplace=True)\n",
    "        self.data = (self.data - self.data.mean()) / self.data.std()\n",
    "        self.data['d'] = np.where(self.data['r'] > 0, 1, 0)\n",
    "    def _get_state(self):\n",
    "        return self.data[self.features].iloc[\n",
    "            self.bar - self.osn:self.bar].values\n",
    "    def seed(self, seed=None):\n",
    "        pass\n",
    "    def reset(self):\n",
    "        self.treward = 0\n",
    "        self.accuracy = 0\n",
    "        self.bar = self.osn\n",
    "        state = self.data[self.features].iloc[\n",
    "            self.bar - self.osn:self.bar]\n",
    "        return state.values\n",
    "    def step(self, action):\n",
    "        correct = action == self.data['d'].iloc[self.bar]\n",
    "        reward = 1 if correct else 0\n",
    "        self.treward += reward\n",
    "        self.bar += 1\n",
    "        self.accuracy = self.treward / (self.bar - self.osn)\n",
    "        if self.bar >= len(self.data):\n",
    "            done = True\n",
    "        elif reward == 1:\n",
    "            done = False\n",
    "        elif (self.accuracy < self.min_accuracy and\n",
    "              self.bar > self.osn + 10):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        state = self._get_state()\n",
    "        info = {}\n",
    "        return state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'symbol' and 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/afirez/studio/python/auto-trade/examples/py_quant/trade/backtest/backtest_01_drl/backtest_01_drl.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/afirez/studio/python/auto-trade/examples/py_quant/trade/backtest/backtest_01_drl/backtest_01_drl.ipynb#ch0000023?line=0'>1</a>\u001b[0m env \u001b[39m=\u001b[39m Finance()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'symbol' and 'features'"
     ]
    }
   ],
   "source": [
    "symbol = 'EUR='\n",
    "env = Finance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体的实现DRL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/afirez/studio/python/auto-trade/examples/py_quant/trade/backtest/backtest_01_drl/backtest_01_drl.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/afirez/studio/python/auto-trade/examples/py_quant/trade/backtest/backtest_01_drl/backtest_01_drl.ipynb#ch0000020?line=2'>3</a>\u001b[0m     np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(seed)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/afirez/studio/python/auto-trade/examples/py_quant/trade/backtest/backtest_01_drl/backtest_01_drl.ipynb#ch0000020?line=3'>4</a>\u001b[0m     env\u001b[39m.\u001b[39mseed(seed)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/afirez/studio/python/auto-trade/examples/py_quant/trade/backtest/backtest_01_drl/backtest_01_drl.ipynb#ch0000020?line=5'>6</a>\u001b[0m set_seeds(\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/afirez/studio/python/auto-trade/examples/py_quant/trade/backtest/backtest_01_drl/backtest_01_drl.ipynb#ch0000020?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m deque\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/afirez/studio/python/auto-trade/examples/py_quant/trade/backtest/backtest_01_drl/backtest_01_drl.ipynb#ch0000020?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam, RMSprop\n",
      "\u001b[1;32m/Users/afirez/studio/python/auto-trade/examples/py_quant/trade/backtest/backtest_01_drl/backtest_01_drl.ipynb Cell 13'\u001b[0m in \u001b[0;36mset_seeds\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/afirez/studio/python/auto-trade/examples/py_quant/trade/backtest/backtest_01_drl/backtest_01_drl.ipynb#ch0000020?line=1'>2</a>\u001b[0m random\u001b[39m.\u001b[39mseed(seed)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/afirez/studio/python/auto-trade/examples/py_quant/trade/backtest/backtest_01_drl/backtest_01_drl.ipynb#ch0000020?line=2'>3</a>\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(seed)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/afirez/studio/python/auto-trade/examples/py_quant/trade/backtest/backtest_01_drl/backtest_01_drl.ipynb#ch0000020?line=3'>4</a>\u001b[0m env\u001b[39m.\u001b[39mseed(seed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "def set_seeds(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.seed(seed)\n",
    "\n",
    "set_seeds(100)\n",
    "\n",
    "from collections import deque\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "\n",
    "\n",
    "class DQLAgent:\n",
    "    def __init__(self, gamma=0.95, hu=24, opt=Adam,\n",
    "                 lr=0.001, finish=False):\n",
    "        self.finish = finish\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = 32\n",
    "        self.max_treward = 0\n",
    "        self.averages = list()\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.osn = env.observation_space.shape[0]\n",
    "        self.model = self._build_model(hu, opt, lr)\n",
    "\n",
    "    def _build_model(self, hu, opt, lr):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hu, input_dim=self.osn,\n",
    "                        activation='relu'))\n",
    "        model.add(Dense(hu, activation='relu'))\n",
    "        model.add(Dense(env.action_space.n, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=opt(lr=lr))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        action = self.model.predict(state)[0]\n",
    "        return np.argmax(action)\n",
    "\n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            if not done:\n",
    "                reward += self.gamma * np.amax(\n",
    "                    self.model.predict(next_state)[0])\n",
    "            target = self.model.predict(state)\n",
    "            target[0, action] = reward\n",
    "            self.model.fit(state, target, epochs=1,\n",
    "                           verbose=False)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def learn(self, episodes):\n",
    "        trewards = []\n",
    "        for e in range(1, episodes + 1):\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, self.osn])\n",
    "            for _ in range(5000):\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state,\n",
    "                                        [1, self.osn])\n",
    "                self.memory.append([state, action, reward,\n",
    "                                    next_state, done])\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    treward = _ + 1\n",
    "                    trewards.append(treward)\n",
    "                    av = sum(trewards[-25:]) / 25\n",
    "                    self.averages.append(av)\n",
    "                    self.max_treward = max(self.max_treward, treward)\n",
    "                    templ = 'episode: {:4d}/{} | treward: {:4d} | '\n",
    "                    templ += 'av: {:6.1f} | max: {:4d}'\n",
    "                    print(templ.format(e, episodes, treward, av,\n",
    "                                       self.max_treward), end='\\r')\n",
    "                    break\n",
    "            if av > 195 and self.finish:\n",
    "                print()\n",
    "                break\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                self.replay()\n",
    "\n",
    "    def test(self, episodes):\n",
    "        trewards = []\n",
    "        for e in range(1, episodes + 1):\n",
    "            state = env.reset()\n",
    "            for _ in range(5001):\n",
    "                state = np.reshape(state, [1, self.osn])\n",
    "                action = np.argmax(self.model.predict(state)[0])\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    treward = _ + 1\n",
    "                    trewards.append(treward)\n",
    "                    print('episode: {:4d}/{} | treward: {:4d}'\n",
    "                          .format(e, episodes, treward), end='\\r')\n",
    "                    break\n",
    "        return trewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQLAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重点关注learn函数：\n",
    "\n",
    "每个episode跑5000次，遇到失败会退出。DRL有重放机制，这个我们系统讲强化学习算法的时候再说。\n",
    "\n",
    "训练这个模型花了22分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1000\n",
    "%time agent.learn(episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 DRL 应用于实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 向量化回测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(s):\n",
    "    return np.reshape(s, learn_env.lags, learn_env.n_featues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(agent, env):\n",
    "    env.min_accurency = 0.0\n",
    "    env.min_performence = 0.0\n",
    "    done = False\n",
    "    env.data['p'] = 0\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = np.argmax(\n",
    "            agent.model.predict(reshape(state))[0,0]\n",
    "        )\n",
    "        position = 1 if action == 1 else -1\n",
    "        env.data.loc[:,'p'].iloc[env.bar] = position\n",
    "        state, reward, done, info = env.step(action)\n",
    "    env.data['s'] = env.data['p'] * env.data['r'] * learn_env.leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = agent.learn_env\n",
    "backtest(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.data[['r','s']].iloc[env.lags:].cumsum().apply(np.exp).plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本内的表现还是不错的，我们再来看样本外的表现情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = finance.Finance(\n",
    "    symbol,\n",
    "    features = learn_env.features,\n",
    "    window = learn_env.window,\n",
    "    lags = learn_env.lags,\n",
    "    min_performence = 0.0,\n",
    "    min_accuracy = 0.0,\n",
    "    start = a + b + c,\n",
    "    end = None,\n",
    "    mu = learn_env.mu,\n",
    "    std = learn_env.std,\n",
    ")\n",
    "backtest(agent, test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env.data[['r','s']].iloc[env.lags:].cumsum().apply(np.exp).plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这里未考虑手续费。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结：\n",
    "\n",
    "本文重在演示强化学习如何作用于时间序列数据，这个方向一定是可行，而且应该深入探讨的。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6869619afde5ccaa692f7f4d174735a0f86b1f7ceee086952855511b0b6edec0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
